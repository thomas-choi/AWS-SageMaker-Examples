{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5317c51c",
   "metadata": {},
   "source": [
    "[Original AWS notebook](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f72ba",
   "metadata": {},
   "source": [
    "Typically a Machine Learning (ML) process consists of few steps: data gathering with various ETL jobs, pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm. In many cases, when the trained model is used for processing real time or batch prediction requests, the model receives data in a format which needs to pre-processed (e.g. featurized) before it can be passed to the algorithm. In the following notebook, we will demonstrate how you can build your ML Pipeline leveraging the Sagemaker Scikit-learn container and SageMaker Linear Learner algorithm & after the model is trained, deploy the Pipeline (Data preprocessing and Lineara Learner) as an Inference Pipeline behind a single Endpoint for real time inference and for batch inferences using Amazon SageMaker Batch Transform.\n",
    "\n",
    "We will demonstrate this using the Abalone Dataset to guess the age of Abalone with physical features. The dataset is available from UCI Machine Learning; the aim for this task is to determine age of an Abalone (a kind of shellfish) from its physical measurements. We’ll use Sagemaker’s Scikit-learn container to featurize the dataset so that it can be used for training with Linear Learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa5de5",
   "metadata": {},
   "source": [
    "Let’s first create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bd225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefix\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"Scikit-LinearLearner-pipeline-abalone-example\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abcfec",
   "metadata": {},
   "source": [
    "### Preprocessing data and training the model\n",
    "SageMaker team has downloaded the dataset from UCI and uploaded to one of the S3 buckets in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b557fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir abalone_data\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/uci_abalone/abalone.csv\",\n",
    "    \"abalone_data/abalone.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02615d1",
   "metadata": {},
   "source": [
    "To run Scikit-learn on Sagemaker **SKLearn** Estimator with a script as an entry point. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "\n",
    "SM_OUTPUT_DIR: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing two input channels, ‘train’ and ‘test’, were used in the call to the Chainer estimator’s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "SM_CHANNEL_TRAIN: A string representing the path to the directory containing data in the ‘train’ channel\n",
    "\n",
    "SM_CHANNEL_TEST: Same as above, but for the ‘test’ channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance. For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d3ffb-64dc-435c-a680-639d4f799b62",
   "metadata": {},
   "source": [
    "To run our Scikit-learn training script on SageMaker, we construct a **sagemaker.sklearn.estimator.sklearn** estimator, which accepts several constructor arguments:\n",
    "\n",
    "* entry_point: The path to the Python script SageMaker runs for training and prediction.\n",
    "* role: Role ARN\n",
    "* framework_version: Scikit-learn version you want to use for executing your model training code.\n",
    "* train_instance_type (optional): The type of SageMaker instances for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* sagemaker_session (optional): The session used to train on Sagemaker.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e80d3a-7c54-4c5b-aece-af4865aafd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"1.2-1\"\n",
    "script_path = \"sklearn_abalone_featurizer.py\"\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba62966-d5bf-497d-aa14-a8d4390d588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_preprocessor.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c3773-d178-4480-aec4-92819d2c4400",
   "metadata": {},
   "source": [
    "Now that our proprocessor is properly fitted, let’s go ahead and preprocess our training data. Let’s use batch transform to directly preprocess the raw data and store right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd82f5c-4800-4019-a1da-2437b4ca055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m5.xlarge\", assemble_with=\"Line\", accept=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3345b4-1a35-46cb-aeb8-4920d99fce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type=\"text/csv\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218754-27c2-48d4-8d30-0041beb70b3b",
   "metadata": {},
   "source": [
    "Let’s take the preprocessed training data and fit a LinearLearner Model. Sagemaker provides prebuilt algorithm containers that can be used with the Python SDK. The previous Scikit-learn job preprocessed the raw Titanic dataset into labeled, useable data that we can now use to fit a binary classifier Linear Learner model.\n",
    "\n",
    "For more on Linear Learner see: https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddafdd-05e7-494e-ab50-4e12f3e5cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "ll_image = retrieve(\"linear-learner\", boto3.Session().region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809610a8-c37e-464c-8cd2-d53d3252860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "s3_ll_output_location = \"s3://{}/{}/{}/{}\".format(\n",
    "    bucket, prefix, s3_ll_output_key_prefix, \"ll_model\"\n",
    ")\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(\n",
    "    ll_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.2xlarge\",\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_ll_output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(feature_dim=10, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "ll_train_data = sagemaker.inputs.TrainingInput(\n",
    "    preprocessed_train,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": ll_train_data}\n",
    "ll_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a4877-45fb-49a1-a9ab-1147fad7dfb4",
   "metadata": {},
   "source": [
    "### Serial Inference Pipeline with Scikit preprocessor and Linear Learner\n",
    "Setting up a Machine Learning pipeline can be done with the Pipeline Model. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model. Deploying the model follows the same **deploy** pattern in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573cfd8-8d79-45e2-b0a2-799098dbcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inferencee_model = sklearn_preprocessor.create_model()\n",
    "linear_learner_model = ll_estimator.create_model()\n",
    "\n",
    "model_name = \"inference-pipeline-\" + timestamp_prefix\n",
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, role=role, models=[scikit_learn_inferencee_model, linear_learner_model]\n",
    ")\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type=\"ml.c4.xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdc110-e149-4962-9a3d-afbb62c7c67a",
   "metadata": {},
   "source": [
    "Here we just grab the first line from the test data (you’ll notice that the inference python script is very particular about the ordering of the inference request data). The **ContentType** field configures the first container, while the **Accept** field configures the last container. You can also specify each container’s \"\"Accept** and **ContentType** values using environment variables.\n",
    "\n",
    "We make our request with the payload in **'text/csv'** format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the output_fn() method in our entry point. Note that we set the Accept to application/json, since Linear Learner does not support text/csv Accept. The prediction output in this case is trying to guess the number of rings the abalone specimen would have given its other physical features; the actual number of rings is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c6743-7340-4bc4-8530-364bd85ddac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155\"\n",
    "actual_rings = 10\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=sagemaker_session, serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452061b3-fd97-4f03-8b1e-8261716745f5",
   "metadata": {},
   "source": [
    "Once we are finished with the endpoint, we clean up the resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46181275-1bab-4b68-b154-22de16c74bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client(\"sagemaker\")\n",
    "predictor.delete_model()\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d8fb7-5791-4cd4-a8e6-54cd17fc7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's no delete model, yet\n",
    "# transformer.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
