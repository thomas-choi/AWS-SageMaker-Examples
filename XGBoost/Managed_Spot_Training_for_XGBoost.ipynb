{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241f7767",
   "metadata": {},
   "source": [
    "This notebook shows usage of SageMaker Managed Spot infrastructure for XGBoost training. Below we show how Spot instances can be used for the ‘algorithm mode’ and ‘script mode’ training methods with the XGBoost container.\n",
    "\n",
    "Managed Spot Training uses Amazon EC2 Spot instance to run training jobs instead of on-demand instances. You can specify which training jobs use spot instances and a stopping condition that specifies how long Amazon SageMaker waits for a job to run using Amazon EC2 Spot instances.\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "In this notebook we will perform XGBoost training as described `here <>`__. See the original notebook for more details on the data.\n",
    "\n",
    "### Setup variables and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "CPU times: user 1.77 s, sys: 148 ms, total: 1.92 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-spot\"\n",
    "# customize to your bucket where you have would like to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b56c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-381492029181\n",
      "sagemaker/DEMO-xgboost-spot\n"
     ]
    }
   ],
   "source": [
    "print(bucket)\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3bb249",
   "metadata": {},
   "source": [
    "### Fetching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77c26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 23.4 ms, total: 419 ms\n",
      "Wall time: 906 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-381492029181/sagemaker/DEMO-xgboost-spot/validation/abalone'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s3 = boto3.client(\"s3\")\n",
    "# Load the dataset\n",
    "FILE_DATA = \"abalone\"\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    f\"datasets/tabular/uci_abalone/abalone.libsvm\",\n",
    "    FILE_DATA,\n",
    ")\n",
    "sagemaker.Session().upload_data(FILE_DATA, bucket=bucket, key_prefix=prefix + \"/train\")\n",
    "sagemaker.Session().upload_data(FILE_DATA, bucket=bucket, key_prefix=prefix + \"/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9881e",
   "metadata": {},
   "source": [
    "### Obtaining the latest XGBoost container\n",
    "\n",
    "We obtain the new container by specifying the framework version (1.7-1). This version specifies the upstream XGBoost framework version (1.7) and an additional SageMaker version (1). If you have an existing XGBoost workflow based on the previous (1.0-1, 1.2-2, 1.3-1 or 1.5-1) container, this would be the only change necessary to get the same workflow working with the new container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be6abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.7-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537edbb",
   "metadata": {},
   "source": [
    "### Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes few minutes.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.xgboost.estimator.XGBoost estimator, which accepts several constructor arguments:\n",
    "\n",
    "* entry_point: The path to the Python script SageMaker runs for training and prediction.\n",
    "* role: Role ARN\n",
    "* hyperparameters: A dictionary passed to the train function as hyperparameters.\n",
    "* train_instance_type (optional): The type of SageMaker instances for training. Note: This particular mode does not currently support training on GPU instance types.\n",
    "* sagemaker_session (optional): The session used to train on Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17febb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"abalone-xgb\")\n",
    "content_type = \"libsvm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce79780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-381492029181/sagemaker/DEMO-xgboost-spot/abalone-xgb/output\n"
     ]
    }
   ],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a996ddb",
   "metadata": {},
   "source": [
    "If Spot instances are used, the training job can be interrupted, causing it to take longer to start or finish. If a training job is interrupted, a checkpointed snapshot can be used to resume from a previously saved point and can save training time (and cost).\n",
    "\n",
    "To enable checkpointing for Managed Spot Training using SageMaker XGBoost we need to configure three things:\n",
    "\n",
    "1. Enable the **train_use_spot_instances** constructor arg - a simple self-explanatory boolean.\n",
    "\n",
    "2. Set the **train_max_wait constructor** arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you’re only charged for actual compute time spent once Spot instances have been successfully procured.\n",
    "\n",
    "3. Setup a **checkpoint_s3_uri** constructor arg - this arg will tell SageMaker an S3 location where to save checkpoints. While not strictly necessary, checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don’t lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the **train_use_spot_instances** variable to see the effect of running the same job using regular (a.k.a. “On Demand”) infrastructure.\n",
    "\n",
    "Note that **train_max_wait** can be set if and only if **train_use_spot_instances** is enabled and must be greater than or equal to **train_max_run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec577ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job DEMO-xgboost-spot-2024-09-12-04-18-15\n",
      "Checkpoint path: s3://sagemaker-us-east-1-381492029181/sagemaker/DEMO-xgboost-spot/checkpoints/DEMO-xgboost-spot-2024-09-12-04-18-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: DEMO-xgboost-spot-2024-09-12-04-18-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in data: s3://sagemaker-us-east-1-381492029181/sagemaker/DEMO-xgboost-spot/train\n",
      "2024-09-12 04:18:17 Starting - Starting the training job...\n",
      "2024-09-12 04:18:30 Starting - Preparing the instances for training...\n",
      "2024-09-12 04:19:08 Downloading - Downloading the training image......\n",
      "2024-09-12 04:20:14 Training - Training image download completed. Training in progress.\n",
      "2024-09-12 04:20:14 Uploading - Uploading generated training model\u001b[34m[2024-09-12 04:20:10.304 ip-10-2-203-119.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.325 ip-10-2-203-119.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] creating symlink between Path /opt/ml/input/data/train/abalone and destination /tmp/sagemaker_xgboost_input_data/abalone5845697049224967236\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Train matrix has 4177 rows and 9 columns\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.720 ip-10-2-203-119.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.721 ip-10-2-203-119.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.723 ip-10-2-203-119.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-09-12:04:20:10:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.09086\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.730 ip-10-2-203-119.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-09-12 04:20:10.731 ip-10-2-203-119.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.61128\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.44558\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.54893\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 48 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.85380\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.32450\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.92907\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.64925\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.43828\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 48 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.28504\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:2.17757\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:2.10257\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.04681\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.00737\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:1.97778\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:1.95060\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:1.93036\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.91997\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.90255\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.88461\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.87660\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.86282\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.85499\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.84877\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.84014\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.83703\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.82825\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.82615\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.81786\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.81118\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:1.80298\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:1.79704\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:1.78973\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:1.78096\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1.77939\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:1.77711\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.77266\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:1.76878\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:1.76343\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:1.75774\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:1.75110\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:1.74668\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:1.74404\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:1.74232\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:1.73694\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:1.73464\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:1.72677\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:1.72361\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:1.71716\u001b[0m\n",
      "\u001b[34m[04:20:10] INFO: ../src/tree/updater_prune.cc:98: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:1.70623\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-09-12 04:20:27 Completed - Training job completed\n",
      "Training seconds: 95\n",
      "Billable seconds: 32\n",
      "Managed Spot Training savings: 66.3%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "job_name = \"DEMO-xgboost-spot-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None\n",
    "checkpoint_s3_uri = (\n",
    "    \"s3://{}/{}/checkpoints/{}\".format(bucket, prefix, job_name) if use_spot_instances else None\n",
    ")\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=5,  # 5 GB\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    ")\n",
    "\n",
    "in_data = \"s3://{}/{}/{}\".format(bucket, prefix, \"train\")\n",
    "print(\"in data:\", in_data)\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=in_data, content_type=\"libsvm\"\n",
    ")\n",
    "estimator.fit({\"train\": train_input}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea07e3",
   "metadata": {},
   "source": [
    "## Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "* **Training seconds: X** : This is the actual compute-time your training job spent\n",
    "* **Billable seconds: Y** : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the **train_use_spot_instances**, then you should see a notable difference between X and Y signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line: - **Managed Spot Training savings: (1-Y/X)*100 %**\n",
    "\n",
    "### Train with Automatic Model Tuning (HPO) and Spot Training enabled\n",
    "You could also train with Amazon SageMaker Automatic Model Tuning. AMT, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a HyperparameterTuner object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "\n",
    "The code sample below shows you how to use the HyperParameterTuner and Spot Training together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cd351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: DEMO-xgboost-spot-20-240912-0449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"max_depth\": IntegerParameter(0, 10, scaling_type=\"Auto\"),\n",
    "    \"num_round\": IntegerParameter(1, 4000, scaling_type=\"Auto\"),\n",
    "    \"alpha\": ContinuousParameter(0, 2, scaling_type=\"Auto\"),\n",
    "    \"subsample\": ContinuousParameter(0.5, 1, scaling_type=\"Auto\"),\n",
    "    \"min_child_weight\": ContinuousParameter(0, 120, scaling_type=\"Auto\"),\n",
    "    \"gamma\": ContinuousParameter(0, 5, scaling_type=\"Auto\"),\n",
    "    \"eta\": ContinuousParameter(0.1, 0.5, scaling_type=\"Auto\"),\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2\n",
    "\n",
    "hp_tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    \"validation:rmse\",\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=max_jobs,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    "    objective_type=\"Minimize\",\n",
    "    base_tuning_job_name=job_name,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "# In this case, the tuner requires a `validation` channel to emit the validation:rmse metric.\n",
    "# Since we only created a `train` channel, we re-use it for validation.\n",
    "hp_tuner.fit({\"train\": train_input, \"validation\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da0d45",
   "metadata": {},
   "source": [
    "### Enabling checkpointing for script mode\n",
    "An additional mode of operation is to run customizable scripts as part of the training and inference jobs. See [this notebook](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.html) for details on how to setup script mode.\n",
    "\n",
    "Here we highlight the specific changes that would enable checkpointing and use Spot instances.\n",
    "\n",
    "Checkpointing in the framework mode for SageMaker XGBoost can be performed using two convenient functions:\n",
    "\n",
    "* `save_checkpoint`:** this returns a callback function that performs checkpointing of the model for each round. This is passed to XGBoost as part of the `callbacks` <https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train>`  argument.\n",
    "* `load_checkpoint`: This is used to load existing checkpoints to ensure training resumes from where it previously stopped.\n",
    "\n",
    "Both functions take the checkpoint directory as input, which in the below example is set to `/opt/ml/checkpoints`. The primary arguments that change for the `xgb.train` call are\n",
    "\n",
    "1. `xgb_model`: This refers to the previous checkpoint (saved from a previously run partial job) obtained by `load_checkpoint`. This would be None if no previous checkpoint is available.\n",
    "2. `callbacks`: This contains a function that performs the checkpointing\n",
    "\n",
    "Updated script looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6dd97e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m CHECKPOINTS_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/opt/ml/checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# default location for Checkpoints\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\u001b[43msave_checkpoint\u001b[49m(CHECKPOINTS_DIR)]\n\u001b[1;32m      3\u001b[0m prev_checkpoint, n_iterations_prev_run \u001b[38;5;241m=\u001b[39m load_checkpoint(CHECKPOINTS_DIR)\n\u001b[1;32m      4\u001b[0m bst \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      5\u001b[0m         params\u001b[38;5;241m=\u001b[39mtrain_hp,\n\u001b[1;32m      6\u001b[0m         dtrain\u001b[38;5;241m=\u001b[39mdtrain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "CHECKPOINTS_DIR = '/opt/ml/checkpoints'   # default location for Checkpoints\n",
    "callbacks = [save_checkpoint(CHECKPOINTS_DIR)]\n",
    "prev_checkpoint, n_iterations_prev_run = load_checkpoint(CHECKPOINTS_DIR)\n",
    "bst = xgb.train(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=(args.num_round - n_iterations_prev_run),\n",
    "        xgb_model=prev_checkpoint,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573033f",
   "metadata": {},
   "source": [
    "### Using the SageMaker XGBoost Estimator\n",
    "The XGBoost estimator class in the SageMaker Python SDK allows us to run that script as a training job on the Amazon SageMaker managed training infrastructure. We’ll also pass the estimator our IAM role, the type of instance we want to use, and a dictionary of the hyperparameters that we want to pass to our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd54716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.4xlarge.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job DEMO-xgboost-regression-2024-09-12-06-39-41\n",
      "Checkpoint path: s3://sagemaker-us-east-1-381492029181/sagemaker/DEMO-xgboost-spot/checkpoints/DEMO-xgboost-regression-2024-09-12-06-39-41\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "job_name = \"DEMO-xgboost-regression-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "checkpoint_s3_uri = (\n",
    "    \"s3://{}/{}/checkpoints/{}\".format(bucket, prefix, job_name) if use_spot_instances else None\n",
    ")\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(\n",
    "    entry_point=\"abalone.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=\"1.7-1\",\n",
    "    output_path=\"s3://{}/{}/{}/output\".format(bucket, prefix, \"xgboost-script-mode\"),\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77bfb5",
   "metadata": {},
   "source": [
    "Training is as simple as calling **fit** on the Estimator. This will start a SageMaker Training job that will download the data, invoke the entry point code (in the provided script file), and save any model artifacts that the script creates. In this case, the script requires a **train** and a **validation** channel. Since we only created a **train** channel, we re-use it for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da1edad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'abalone.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxgb_script_mode_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1344\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     experiment_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m ):\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model using the input training dataset.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    The API calls the Amazon SageMaker CreateTrainingJob API to start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m _TrainingJob\u001b[38;5;241m.\u001b[39mstart_new(\u001b[38;5;28mself\u001b[39m, inputs, experiment_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:3559\u001b[0m, in \u001b[0;36mFramework._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\u001b[39;00m\n\u001b[1;32m   3553\u001b[0m \n\u001b[1;32m   3554\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3557\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n\u001b[1;32m   3558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3559\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFramework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3561\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_set_debugger_configs()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:942\u001b[0m, in \u001b[0;36mEstimatorBase._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39ms3_prefix\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stage_user_code_in_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m     code_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39ms3_prefix\n\u001b[1;32m    944\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39mscript_name\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1024\u001b[0m, in \u001b[0;36mEstimatorBase._stage_user_code_in_s3\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             output_bucket, _ \u001b[38;5;241m=\u001b[39m parse_s3_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_path)\n\u001b[1;32m   1022\u001b[0m             kms_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_kms_key \u001b[38;5;28;01mif\u001b[39;00m code_bucket \u001b[38;5;241m==\u001b[39m output_bucket \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtar_and_upload_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_bucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_s3_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscript\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/fw_utils.py:456\u001b[0m, in \u001b[0;36mtar_and_upload_dir\u001b[0;34m(session, bucket, s3_key_prefix, script, directory, dependencies, kms_key, s3_resource, settings)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     source_files \u001b[38;5;241m=\u001b[39m _list_files_to_compress(script, directory) \u001b[38;5;241m+\u001b[39m dependencies\n\u001b[0;32m--> 456\u001b[0m     tar_file \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tar_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_TAR_SOURCE_FILENAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kms_key:\n\u001b[1;32m    461\u001b[0m         extra_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServerSideEncryption\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws:kms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSEKMSKeyId\u001b[39m\u001b[38;5;124m\"\u001b[39m: kms_key}\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/utils.py:461\u001b[0m, in \u001b[0;36mcreate_tar_file\u001b[0;34m(source_files, target)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, dereference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sf \u001b[38;5;129;01min\u001b[39;00m source_files:\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;66;03m# Add all files from the directory into the root of the directory structure of the tar\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/tarfile.py:2164\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, name)\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;66;03m# Create a TarInfo object from the file.\u001b[39;00m\n\u001b[0;32m-> 2164\u001b[0m tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettarinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarfile: Unsupported type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/tarfile.py:2039\u001b[0m, in \u001b[0;36mTarFile.gettarinfo\u001b[0;34m(self, name, arcname, fileobj)\u001b[0m\n\u001b[1;32m   2037\u001b[0m         statres \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(name)\n\u001b[1;32m   2038\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2039\u001b[0m         statres \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2041\u001b[0m     statres \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(fileobj\u001b[38;5;241m.\u001b[39mfileno())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'abalone.py'"
     ]
    }
   ],
   "source": [
    "xgb_script_mode_estimator.fit({\"train\": train_input, \"validation\": train_input}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20090415",
   "metadata": {},
   "source": [
    "As previously stated, the estimator can also be passed to the HyperparameterTuner object to interact with the Amazon SageMaker hyperparameter tuning APIs and create a HyperParameter Tuning Job. Hyper Parameters are automatically tuned which in most cases results in a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tuner = HyperparameterTuner(\n",
    "    xgb_script_mode_estimator,\n",
    "    \"validation:rmse\",\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=max_jobs,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    "    objective_type=\"Minimize\",\n",
    "    base_tuning_job_name=job_name,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "# In this case, the tuner requires a `validation` channel to emit the validation:rmse metric.\n",
    "# Since we only created a `train` channel, we re-use it for validation.\n",
    "hp_tuner.fit({\"train\": train_input, \"validation\": train_input})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
